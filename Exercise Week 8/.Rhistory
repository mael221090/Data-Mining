m<-data.frame()
m<-cbind(x1,y1,c1)
x2<-seq(from=10, length.out=12, by=-0.3)
y2<-seq(from=7, length.out=12, by=-0.3)
c2<-c(-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1)
for(i in 1:length(x2)){
m <- rbind(m, x2[i])
m[i + 13,2] <- y2[i]
m[i + 13,3] <- c2[i]
}
plot(m[,-3], main='Test', pch=ifelse(m[3] > 0, 1, 2))
m[3]
plot(m[,-3], main='Test', pch=ifelse(m[,3] > 0, 1, 2))
abline(1,1)
x<-seq(from=0.1, length.out=13, by=0.3)
y<-seq(from=3, length.out=13, by=0.7)
c1<-c(1,1,1,1,1,1,1,1,1,1,1,1,1)
m<-data.frame()
m<-cbind(x1,y1,c1)
x2<-seq(from=10, length.out=12, by=-0.3)
y2<-seq(from=7, length.out=12, by=-0.3)
c2<-c(-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1)
for(i in 1:length(x2)){
m <- rbind(m, x2[i])
m[i + 13,2] <- y2[i]
m[i + 13,3] <- c2[i]
}
plot(m[,-3], main='Linearly separable dataset', pch=ifelse(m[,3] > 0, 1, 2))
abline(1,1)
View(m)
x<-seq(from=0.1, length.out=13, by=0.3)
y<-seq(from=3, length.out=13, by=0.7)
class<-c(1,1,1,1,1,1,1,1,1,1,1,1,1)
m<-data.frame()
m<-cbind(x,y,c)
x2<-seq(from=10, length.out=12, by=-0.3)
y2<-seq(from=7, length.out=12, by=-0.3)
c2<-c(-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1)
for(i in 1:length(x2)){
m <- rbind(m, x2[i])
m[i + 13,2] <- y2[i]
m[i + 13,3] <- c2[i]
}
plot(m[,-3], main='Linearly separable dataset', pch=ifelse(m[,3] > 0, 1, 2))
abline(1,1)
x<-seq(from=0.1, length.out=13, by=0.3)
y<-seq(from=3, length.out=13, by=0.7)
class<-c(1,1,1,1,1,1,1,1,1,1,1,1,1)
m<-data.frame()
m<-cbind(x,y,class)
x2<-seq(from=10, length.out=12, by=-0.3)
y2<-seq(from=7, length.out=12, by=-0.3)
c2<-c(-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1)
for(i in 1:length(x2)){
m <- rbind(m, x2[i])
m[i + 13,2] <- y2[i]
m[i + 13,3] <- c2[i]
}
x<-seq(from=0.1, length.out=13, by=0.3)
y<-seq(from=3, length.out=13, by=0.7)
class<-c(1,1,1,1,1,1,1,1,1,1,1,1,1)
m<-data.frame()
m<-cbind(x,y,class)
x2<-seq(from=10, length.out=12, by=-0.3)
y2<-seq(from=7, length.out=12, by=-0.3)
c2<-c(-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1)
for(i in 1:length(x2)){
m <- rbind(m, x2[i])
m[i + 13,2] <- y2[i]
m[i + 13,3] <- c2[i]
}
plot(m[,-3], main='Linearly separable dataset', pch=ifelse(m[,3] > 0, 1, 2))
abline(1,1)
w <- perceptron(m[,-3], 5, m[3])
View(m)
w <- perceptron(m[,-3], 5, m[,3])
perceptron=function(D, eta, class){
r <- data.frame()
b <- 0.1
converged <- FALSE
w <- rep(1,(ncol(D) - 1))
while(!converged){
converged <- TRUE
for(i in 1:nrow(D)){
a <- sum(w*D[i,]) + b
# output y != input class(y)
if(class[i]*a <= 0){
w <- w + class[i]*eta*D[i,]
r <- rbind(r, w)
converged <- FALSE
}
}
}
return (r)
}
w <- perceptron(m[,-3], 5, m[,3])
View(w)
w <- perceptron(m[,-3], 5, m[,3])
plot(m[,-3], main='Linearly separable dataset', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
abline(-b/w2, -w1/w2)
}
x <- c(6.5, 5.1, 3.6, 7.9, 4.1, 3.7, 3.4, 8.0, 5.5, 7.0, 4.3, 6.6, 12)
y <- c(2.2, 3, 2.9, 5.4, 7.1, 8.9, 8, 11.3, 9.0, 9.9, 6.6, 8.9, 15)
class<-c(1,1,1,1,1,1,1,1,1,1,1,1,1)
m<-data.frame()
m<-cbind(x,y,class)
plot(x,y)
x <- c(2.2, 5.1, 3.6, 7.9, 4.1, 3.7, 3.4, 8.0, 5.5, 7.0, 4.3, 6.6, 12)
y <- c(6.5, 3, 2.9, 5.4, 7.1, 8.9, 8, 11.3, 9.0, 9.9, 6.6, 8.9, 15)
class<-c(1,1,1,1,1,1,1,1,1,1,1,1,1)
m<-data.frame()
m<-cbind(x,y,class)
plot(x,y)
x <- c(2.2, 3, 1.1, 7.9, 4.1, 3.7, 3.4, 8.0, 5.5, 7.0, 4.3, 6.6, 12)
y <- c(6.5, 5.1, 2.9, 5.4, 7.1, 8.9, 8, 11.3, 9.0, 9.9, 6.6, 8.9, 15)
class<-c(1,1,1,1,1,1,1,1,1,1,1,1,1)
m<-data.frame()
m<-cbind(x,y,class)
plot(x,y)
x <- c(2.2, 3, 1.1, 7.9, 4.1, 3.7, 3.4, 2.0, 5.5, 7.0, 4.3, 6.6, 12)
y <- c(6.5, 5.1, 2.9, 5.4, 7.1, 8.9, 8, 11.3, 9.0, 9.9, 6.6, 8.9, 15)
class<-c(1,1,1,1,1,1,1,1,1,1,1,1,1)
m<-data.frame()
m<-cbind(x,y,class)
plot(x,y)
x <- c(2.2, 3, 1.1, 7.9, 4.1, 3.7, 3.4, 2.0, 5.5, 7.0, 4.3, 6.6, 12)
y <- c(6.5, 5.1, 2.9, 5.4, 7.1, 8.9, 8, 11.3, 9.0, 9.9, 6.6, 8.9, 15)
class<-c(1,1,1,1,1,1,1,1,1,1,1,1,1)
m<-data.frame()
m<-cbind(x,y,class)
plot(x,y)
x <- c(2.2, 3, 1.1, 7.9, 4.1, 3.7, 3.4, 2.0, 5.5, 7.0, 4.3, 6.6, 12)
y <- c(6.5, 5.1, 2.9, 9.4, 7.1, 8.9, 8, 11.3, 9.0, 9.9, 6.6, 8.9, 15)
class<-c(1,1,1,1,1,1,1,1,1,1,1,1,1)
m<-data.frame()
m<-cbind(x,y,class)
plot(x,y)
x2<-c(1,2,3,4.2,4.5,5.0, 5.7, 6.2,6.6,7.0)
y2<-c(0.1,0.9,1.1,2.5,3.0,3.1,3.6,4.5,5.7,6.6)
c2<-c(-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1)
plot(x2,y2)
for(i in 1:length(x2)){
m <- rbind(m, x2[i])
m[i + 13,2] <- y2[i]
m[i + 13,3] <- c2[i]
}
plot(m[,-3], main='Linearly separable dataset', pch=ifelse(m[,3] > 0, 1, 2))
abline(1,1)
w <- perceptron(m[,-3], 0.1, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
abline(-b/w2, -w1/w2)
}
w <- perceptron(m[,-3], 0.1, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
w <- perceptron(m[,-3], 0.5, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
w
w <- perceptron(m[,-3], 0.2, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
w <- perceptron(m[,-3], 0.3, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
w <- perceptron(m[,-3], 0.4, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
w <- perceptron(m[,-3], 0.5, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
w <- perceptron(m[,-3], 0.6, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
w <- perceptron(m[,-3], 0.9, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
w <- perceptron(m[,-3], 5, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
perceptron=function(D, eta, class){
r <- data.frame()
b <- 1
converged <- FALSE
w <- rep(1,(ncol(D) - 1))
while(!converged){
converged <- TRUE
for(i in 1:nrow(D)){
a <- sum(w*D[i,]) + b
# output y != input class(y)
if(class[i]*a <= 0){
w <- w + class[i]*eta*D[i,]
r <- rbind(r, w)
converged <- FALSE
}
}
}
return (r)
}
plot(m[,-3], main='Linearly separable dataset', pch=ifelse(m[,3] > 0, 1, 2))
abline(1,1)
legend('bottomright')
legend('bottomright', legend=c('1', '-1'))
legend('bottomright', legend=c('1', '-1'), pch=c('1', '2'))
legend('bottomright', legend=c('1', '-1'), pch='12')
legend('bottomright', legend=c('1', '-1'), pch=c(1,2))
w <- perceptron(m[,-3], 5, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
legend('bottomright', legend=c('1', '-1'), pch=c(1,2))
w <- perceptron(m[,-3], 0.2, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
legend('bottomright', legend=c('1', '-1'), pch=c(1,2))
w <- perceptron(m[,-3], 0.1, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
legend('bottomright', legend=c('1', '-1'), pch=c(1,2))
perceptron=function(D, eta, class){
r <- data.frame()
b <- 0.1
converged <- FALSE
w <- rep(1,(ncol(D) - 1))
while(!converged){
converged <- TRUE
for(i in 1:nrow(D)){
a <- sum(w*D[i,]) + b
# output y != input class(y)
if(class[i]*a <= 0){
w <- w + class[i]*eta*D[i,]
r <- rbind(r, w)
converged <- FALSE
}
}
}
return (r)
}
w <- perceptron(m[,-3], 0.1, m[,3])
plot(m[,-3], main='Perceptron classifier', pch=ifelse(m[,3] > 0, 1, 2))
for(i in 1:nrow(w)){
w1 <- w[i,1]
w2 <- w[i,2]
b <- 0.1
if(i != nrow(w))
abline(-b/w2, -w1/w2,col='green')
else
abline(-b/w2, -w1/w2, col='red')
}
legend('bottomright', legend=c('1', '-1'), pch=c(1,2))
iris
bla <- c(0.1,0.5,1,2,5,8)
for(i in bla){
w <- perceptron(train[,-3], i, train$Species)
res <- rbind(res, w)
}
row.names(res) <- NULL
res
iris
pairs(iris[1:4], main = "Anderson's Iris Data -- 3 species", pch=21, bg = c("red", "green3", "blue")[unclass(iris$Species)])
d <- iris[iris$Species == c('setosa', 'versicolor'), -(2:3)]
row.names(d) <- NULL
d$Species <- as.numeric(d$Species == "setosa")
d$Species[d$Species == 0] <- -1
training <- sample(1:nrow(d), nrow(d), replace=FALSE)
train <- subset(d[training,])
row.names(train) <- NULL
res <- data.frame()
bla <- c(0.1,0.5,1,2,5,8)
for(i in bla){
w <- perceptron(train[,-3], i, train$Species)
res <- rbind(res, w)
}
row.names(res) <- NULL
setwd('~/Desktop//Courses/Warwick//Data Mining/Exercise Week 8/')
require(e1071)
require(randomForest)
data = read.csv('AI2013_papers.csv')
d <- data
d <- d[,-1]
row.names(d) <- NULL
myNormalise=function(data, col){
tmp = sqrt(sum(data[,col]^2))
for(i in 1:nrow(data)){
data[i,col] = data[i,col]/tmp
}
return(data)
}
for(attribute in 1:length(names(d))){
if(is.numeric(d[,attribute]))
{
d <- myNormalise(d, attribute)
}
}
classes <- c('Case Study', 'Correspondence', 'Essay', 'Opinion', 'Perspective', 'Research', 'Review', 'Viewpoint')
#function that implements the three algorithms to be compared
func_NB=function(fold, data){
fitNaive = naiveBayes(type ~., data=data[data$fold != fold,-13])
predictionsNaive = predict(fitNaive, data[data$fold == fold, -c(12,13)])
return(table(predictionsNaive, data[data$fold==fold,]$type))
}
func_SVM=function(fold, data){
fitSVM <- svm(type ~., data = data[data$fold != fold,-13])
predictionsSVM = predict(fitSVM, data[data$fold == fold, -c(12,13)])
return(table(predictionsSVM, data[data$fold==fold,]$type))
}
func_RF=function(fold, data){
fitRandomForest <- randomForest(type ~., data = data[data$fold != fold,-13], importance=TRUE)
predictionsRF = predict(fitRandomForest, data[data$fold == fold, -c(12,13)])
return(table(predictionsRF, data[data$fold==fold,]$type))
}
#get Precision, Recall, F-measure, Accuracy, Rmacro, Rmicro, Pmacro and Pmicro for a fold for a classifier
getPRF = function(table){
tab.precision <- c()
tab.recall <- c()
fmeasure <- c()
acc <- c()
Rmacro <- 0
Rmicro <- 0
Pmicro <- 0
Pmacro <- 0
TPc <- 0
for(i in 1:ncol(table)){
TP <- table[i,i]
FN <- sum(table[-i,i])
FP <- sum(table[i,-i])
TN <- sum(table[-i,-i])
precision <- TP/(TP+FP)
recall <- TP/(TP+FN)
f <- 2*precision*recall/(precision+recall)
accuracy <- (TP + TN)/(TP + TN + FP + FN)
tab.precision <- append(tab.precision, precision, after=length(tab.precision))
tab.recall <- append(tab.recall, recall, after=length(tab.recall))
fmeasure <- append(fmeasure, f, after=length(fmeasure))
acc <- append(acc, accuracy, after=length(acc))
Rmacro <- Rmacro + recall
Pmacro <- Pmacro + precision
TPc <- TPc + TP
Rmicro <- TP + FN
Pmicro <- TP + FP
}
Rmacro <- Rmacro/ncol(table)
Pmacro <- Pmacro/ncol(table)
Rmicro <- TPc / Rmicro
Pmicro <- TPc / Pmicro
return(data.frame(tab.precision, tab.recall, fmeasure, acc, Rmacro, Pmacro, Rmicro, Pmicro, row.names=classes))
}
#function that implements the cross-validation over a specific algorithm
doCV = function(data, kfold, algo){
l=list()
for (i in 1:kfold) {
df = as.data.frame(getPRF(algo(i, data)))
df[is.na(df)] <- 0
l[[i]] = df
}
return (l)
}
d2<-d[sample(nrow(d)),]
d2$fold = cut(1:nrow(d2), breaks=10, labels=F)
row.names(d2) <- NULL
lNaive = doCV(d2, 10, func_NB)
lNaive
lSVM = doCV(d2, 10, func_SVM)
lSVM
lRF = doCV(d2, 10, func_RF)
lRF
#get the average accuracy for each fold and each classifier
getAverage = function(l){
folds <- c('Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5', 'Fold 6', 'Fold 7', 'Fold 8', 'Fold 9', 'Fold 10', 'Average')
x <- numeric(11)
df <- data.frame(x,x,x,x,x,x,x,x, row.names=folds)
colnames(df) <- c('Precision', 'Recall', 'Fmeasure', 'Accuracy', 'Rmacro', 'Pmacro', 'Rmicro', 'Pmicro')
#browse by column
for(j in 1:dim(l[[1]])[1]){
#browse by folds
for(i in 1:length(l)){
df[i,j] <- sapply(l[[i]][j], mean)
}
}
df[11,] <- apply(df[-11,], 2, mean)
return(df)
}
dfNaive <- getAverage(lNaive)
dfNaive
dfSVM <- getAverage(lSVM)
dfSVM
dfRF <- getAverage(lRF)
dfRF
getTvalue = function(dfNaive, dfSVM, dfRF){
folds <- c('Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5', 'Fold 6', 'Fold 7', 'Fold 8', 'Fold 9', 'Fold 10', 'Average', 'Stdev', 't-value')
cols <- c('NB-SVM', 'NB-RF', 'SVM-RF')
x <- numeric(13)
df <- data.frame(x,x,x, row.names=folds)
#extract the average and standard deviation of each of the three classfiers to calculate the t-test
for(i in 1:nrow(dfNaive)){
df[i,1] <- dfNaive[i,4] - dfSVM[i,4]
df[i,2] <- dfNaive[i,4] - dfRF[i,4]
df[i,3] <- dfSVM[i,4] - dfRF[i,4]
}
df[11,] <- apply(df[-c(11,12,13),], 2, mean)
df[12,] <- apply(df[-c(11,12,13),], 2, sd)
for(i in 1:ncol(df)){
df[13,i] <- abs(df[11,i]/(df[12,i]/sqrt(10)))
}
colnames(df) <- cols
return (df)
}
dTvalue <- getTvalue(dfNaive, dfSVM, dfRF)
dTvalue
